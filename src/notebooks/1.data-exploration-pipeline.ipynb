{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;font-size:200%;;\"> [Pyspark] Data Exploration and Analysis  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"artifacts/data_ingestion/training.1600000.processed.noemoticon.csv\"\n",
    "OUTPUT_FILE = (\n",
    "    \"artifacts/data_ingestion/training.1600000.processed.noemoticon_cleaned.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apache Spark web UI is usualy:\n",
    "[http://127.0.0.1:4040](http://127.0.0.1:4040)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /app\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/17 19:05:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import PySpark related modules\n",
    "import pyspark\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a spark session.\n",
    "MAX_MEMORY = \"4G\"\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "    .setMaster(\"local[*]\")\n",
    "    .set(\"spark.executor.heartbeatInterval\", 10000)\n",
    "    .set(\"spark.network.timeout\", 10000)\n",
    "    .set(\"spark.core.connection.ack.wait.timeout\", \"3600\")\n",
    "    .set(\"spark.executor.memory\", MAX_MEMORY)\n",
    "    .set(\"spark.driver.memory\", MAX_MEMORY)\n",
    ")\n",
    "# initait ethe sperk session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ETL with Spark RDD\").config(conf=conf).getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data loading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total=1599999 rows\n",
      "Raw data :\n",
      "    0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
      "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
      "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
      "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
      "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
      "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
      "\n",
      "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
      "0  is upset that he can't update his Facebook by ...                                                                   \n",
      "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
      "2    my whole body feels itchy and like its on fire                                                                    \n",
      "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
      "4                      @Kwesidei not the whole crew                                                                    \n"
     ]
    }
   ],
   "source": [
    "# Import PySpark related modules\n",
    "import pyspark.sql.functions as f\n",
    "from utils.spark_utils import init_spark, spark_load_data\n",
    "\n",
    "# initialize the spark sessions\n",
    "spark = init_spark(MAX_MEMORY=\"4G\")\n",
    "\n",
    "# Load the main dataset into pyspark data frame\n",
    "spark_df = spark_load_data(spark, INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_columns_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "set(spark_df.columns) == set(new_columns_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:============================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|target|       ids|                date|    flag|           user|                text|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|     0|1548274671|Fri Apr 17 20:30:...|NO_QUERY|   xoLovebug224|Working on my son...|\n",
      "|     0|1548274782|Fri Apr 17 20:30:...|NO_QUERY|    Kerry_Baker|can't sleep, it's...|\n",
      "|     0|1548275152|Fri Apr 17 20:30:...|NO_QUERY|glamorusindie81|wishing i could b...|\n",
      "|     0|1548275569|Fri Apr 17 20:30:...|NO_QUERY|          WOnet|Well, @LilWO was ...|\n",
      "|     0|1548275799|Fri Apr 17 20:30:...|NO_QUERY|jessicakornberg|taking some much ...|\n",
      "|     0|1548275819|Fri Apr 17 20:30:...|NO_QUERY|        MrzEndy|@latinluvly aww w...|\n",
      "|     0|1548275856|Fri Apr 17 20:30:...|NO_QUERY|      WampusKat|@chriswhill sweet...|\n",
      "|     0|1548276175|Fri Apr 17 20:30:...|NO_QUERY|   thisgoeshere|my boyfriend is g...|\n",
      "|     0|1548276354|Fri Apr 17 20:30:...|NO_QUERY|          Kelsx|I'm losing follow...|\n",
      "|     0|1548276360|Fri Apr 17 20:30:...|NO_QUERY|customcanvasart|@USEOFFORCEENT so...|\n",
      "|     0|1548276329|Fri Apr 17 20:30:...|NO_QUERY|      sexyskier|@gabriel_hermes  ...|\n",
      "|     0|1548276405|Fri Apr 17 20:30:...|NO_QUERY|         DJLM88|       I'm so tired |\n",
      "|     0|1548276577|Fri Apr 17 20:30:...|NO_QUERY|          Pearl|misses updating h...|\n",
      "|     0|1548276552|Fri Apr 17 20:30:...|NO_QUERY|     KristinaaG|@Lorenzohenrie i ...|\n",
      "|     0|1548276887|Fri Apr 17 20:30:...|NO_QUERY|   KuppyKakejEs|Getting hair done...|\n",
      "|     0|1548277056|Fri Apr 17 20:30:...|NO_QUERY|      lorikeety|Andi~sweetheart~j...|\n",
      "|     0|1548276901|Fri Apr 17 20:30:...|NO_QUERY|      zoeydecay|Bored watching La...|\n",
      "|     0|1548277792|Fri Apr 17 20:31:...|NO_QUERY|       leabella|Actually I had 50...|\n",
      "|     0|1548277947|Fri Apr 17 20:31:...|NO_QUERY|     flipflip81|@nick_carter http...|\n",
      "|     0|1548277960|Fri Apr 17 20:31:...|NO_QUERY|      AnitaKoch|@lkhalladay Color...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# assign the coulmns names\n",
    "new_columns_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "spark_df = spark_df.toDF(*new_columns_names)\n",
    "\n",
    "# sort rows by date\n",
    "spark_df = spark_df.orderBy(\"date\")\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1548274671</td>\n",
       "      <td>Fri Apr 17 20:30:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xoLovebug224</td>\n",
       "      <td>Working on my songg for aunt nan.   kinda hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1548274782</td>\n",
       "      <td>Fri Apr 17 20:30:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Kerry_Baker</td>\n",
       "      <td>can't sleep, it's 4.30am and i have to be up a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1548275152</td>\n",
       "      <td>Fri Apr 17 20:30:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>glamorusindie81</td>\n",
       "      <td>wishing i could be at coachella this weekend  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1548275569</td>\n",
       "      <td>Fri Apr 17 20:30:39 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>WOnet</td>\n",
       "      <td>Well, @LilWO was having a tough day/night. Wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1548275799</td>\n",
       "      <td>Fri Apr 17 20:30:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jessicakornberg</td>\n",
       "      <td>taking some much needed naked time.  too bad i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target         ids                          date      flag             user  \\\n",
       "0      0  1548274671  Fri Apr 17 20:30:31 PDT 2009  NO_QUERY     xoLovebug224   \n",
       "1      0  1548274782  Fri Apr 17 20:30:34 PDT 2009  NO_QUERY      Kerry_Baker   \n",
       "2      0  1548275152  Fri Apr 17 20:30:38 PDT 2009  NO_QUERY  glamorusindie81   \n",
       "3      0  1548275569  Fri Apr 17 20:30:39 PDT 2009  NO_QUERY            WOnet   \n",
       "4      0  1548275799  Fri Apr 17 20:30:43 PDT 2009  NO_QUERY  jessicakornberg   \n",
       "\n",
       "                                                text  \n",
       "0  Working on my songg for aunt nan.   kinda hard...  \n",
       "1  can't sleep, it's 4.30am and i have to be up a...  \n",
       "2  wishing i could be at coachella this weekend  ...  \n",
       "3  Well, @LilWO was having a tough day/night. Wan...  \n",
       "4  taking some much needed naked time.  too bad i...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.limit(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==============>                                            (1 + 3) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|target|       ids|                date|    flag|           user|                text|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|     0|1548274671|Fri Apr 17 20:30:...|NO_QUERY|   xoLovebug224|Working on my son...|\n",
      "|     0|1548274782|Fri Apr 17 20:30:...|NO_QUERY|    Kerry_Baker|can't sleep, it's...|\n",
      "|     0|1548275152|Fri Apr 17 20:30:...|NO_QUERY|glamorusindie81|wishing i could b...|\n",
      "|     0|1548275569|Fri Apr 17 20:30:...|NO_QUERY|          WOnet|Well, @LilWO was ...|\n",
      "|     0|1548275799|Fri Apr 17 20:30:...|NO_QUERY|jessicakornberg|taking some much ...|\n",
      "|     0|1548275819|Fri Apr 17 20:30:...|NO_QUERY|        MrzEndy|@latinluvly aww w...|\n",
      "|     0|1548275856|Fri Apr 17 20:30:...|NO_QUERY|      WampusKat|@chriswhill sweet...|\n",
      "|     0|1548276175|Fri Apr 17 20:30:...|NO_QUERY|   thisgoeshere|my boyfriend is g...|\n",
      "|     0|1548276354|Fri Apr 17 20:30:...|NO_QUERY|          Kelsx|I'm losing follow...|\n",
      "|     0|1548276360|Fri Apr 17 20:30:...|NO_QUERY|customcanvasart|@USEOFFORCEENT so...|\n",
      "|     0|1548276329|Fri Apr 17 20:30:...|NO_QUERY|      sexyskier|@gabriel_hermes  ...|\n",
      "|     0|1548276405|Fri Apr 17 20:30:...|NO_QUERY|         DJLM88|       I'm so tired |\n",
      "|     0|1548276577|Fri Apr 17 20:30:...|NO_QUERY|          Pearl|misses updating h...|\n",
      "|     0|1548276552|Fri Apr 17 20:30:...|NO_QUERY|     KristinaaG|@Lorenzohenrie i ...|\n",
      "|     0|1548276887|Fri Apr 17 20:30:...|NO_QUERY|   KuppyKakejEs|Getting hair done...|\n",
      "|     0|1548277056|Fri Apr 17 20:30:...|NO_QUERY|      lorikeety|Andi~sweetheart~j...|\n",
      "|     0|1548276901|Fri Apr 17 20:30:...|NO_QUERY|      zoeydecay|Bored watching La...|\n",
      "|     0|1548277792|Fri Apr 17 20:31:...|NO_QUERY|       leabella|Actually I had 50...|\n",
      "|     0|1548277947|Fri Apr 17 20:31:...|NO_QUERY|     flipflip81|@nick_carter http...|\n",
      "|     0|1548277960|Fri Apr 17 20:31:...|NO_QUERY|      AnitaKoch|@lkhalladay Color...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import data_preparation_pipeline\n",
    "\n",
    "# run the data preparation pipeline\n",
    "spark_df, missing_invalid_df = data_preparation_pipeline(spark, spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_invalid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import data_analysis_pipeline\n",
    "\n",
    "# run the data analysis pipeline\n",
    "(\n",
    "    monthly_sales,\n",
    "    past_sales_stats_df,\n",
    "    current_sales_stats_df,\n",
    "    growth_rate_dict,\n",
    "    top_ranked_clients_df,\n",
    "    worst_ranked_clients_df,\n",
    "    top_purchases_by_gender_df,\n",
    ") = data_analysis_pipeline(spark, spark_df, topN=topN, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_sales_stats_df.round(1).to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_sales_stats_df.round(1).to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ranked_clients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ranked_clients_df.astype(str).to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_ranked_clients_df.astype(str).to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_purchases_by_gender_df.astype(str).to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import data_modeling_pipeline\n",
    "\n",
    "data_modeling_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import model_deployment_pipeline\n",
    "\n",
    "model_deployment_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import save_dict_to_json, load_dict_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 save output for testing and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_json(ratio_gender_dict, filepath=\"data/test/ratio_gender_dict.json\")\n",
    "save_dict_to_json(growth_rate_dict, filepath=\"data/test/growth_rate_dict.json\")\n",
    "save_dict_to_json(clients_dict, filepath=\"data/test/clients_dict.json\")\n",
    "\n",
    "save_dict_to_json(\n",
    "    current_sales.toPandas().astype(str).to_dict(orient=\"records\"),\n",
    "    filepath=\"data/test/current_sales.json\",\n",
    ")\n",
    "save_dict_to_json(\n",
    "    past_sales.toPandas().astype(str).to_dict(orient=\"records\"),\n",
    "    filepath=\"data/test/past_sales.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_json(\n",
    "    past_sales_stats_df.astype(str).to_dict(orient=\"records\"),\n",
    "    filepath=\"data/test/past_sales_stats_df.json\",\n",
    ")\n",
    "save_dict_to_json(\n",
    "    current_sales_stats_df.astype(str).to_dict(orient=\"records\"),\n",
    "    filepath=\"data/test/current_sales_stats_df.json\",\n",
    ")\n",
    "save_dict_to_json(\n",
    "    past_sales.toPandas().astype(str).to_dict(orient=\"records\"),\n",
    "    filepath=\"data/test/past_sales.json\",\n",
    ")\n",
    "\n",
    "save_dict_to_json(\n",
    "    top_ranked_clients_df.astype(str).to_dict(orient=\"records\"),\n",
    "    filepath=\"data/test/top_ranked_clients_df.json\",\n",
    ")\n",
    "save_dict_to_json(\n",
    "    worst_ranked_clients_df.astype(str).to_dict(orient=\"records\"),\n",
    "    filepath=\"data/test/worst_ranked_clients_df.json\",\n",
    ")\n",
    "\n",
    "save_dict_to_json(\n",
    "    top_purchases_by_gender_df.astype(str).to_dict(orient=\"records\"),\n",
    "    filepath=\"data/test/top_purchases_by_gender_df.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_json(\n",
    "    ranked_product_category_df.astype(str).to_dict(orient=\"records\"),\n",
    "    filepath=\"data/test/ranked_product_category_df.json\",\n",
    ")\n",
    "save_dict_to_json(\n",
    "    monthly_stats_spark_df.toPandas().astype(str).to_dict(orient=\"records\"),\n",
    "    filepath=\"data/test/monthly_stats_spark_df.json\",\n",
    ")\n",
    "save_dict_to_json(\n",
    "    yearly_stats_spark_df.toPandas().astype(str).to_dict(orient=\"records\"),\n",
    "    filepath=\"data/test/yearly_stats_spark_df.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rate_dict = load_dict_from_json(filepath=\"data/test/growth_rate_dict.json\")\n",
    "ratio_gender_dict = load_dict_from_json(filepath=\"data/test/ratio_gender_dict.json\")\n",
    "clients_dict = load_dict_from_json(filepath=\"data/test/clients_dict.json\")\n",
    "\n",
    "yearly_stats_df = pd.DataFrame(\n",
    "    load_dict_from_json(filepath=\"data/test/yearly_stats_spark_df.json\")\n",
    ")\n",
    "monthly_stats_df = pd.DataFrame(\n",
    "    load_dict_from_json(filepath=\"data/test/monthly_stats_spark_df.json\")\n",
    ")\n",
    "past_sales_stats_df = pd.DataFrame(\n",
    "    load_dict_from_json(filepath=\"data/test/past_sales_stats_df.json\")\n",
    ")\n",
    "current_sales_stats_df = pd.DataFrame(\n",
    "    load_dict_from_json(filepath=\"data/test/current_sales_stats_df.json\")\n",
    ")\n",
    "top_ranked_clients_df = pd.DataFrame(\n",
    "    load_dict_from_json(filepath=\"data/test/top_ranked_clients_df.json\")\n",
    ")\n",
    "worst_ranked_clients_df = pd.DataFrame(\n",
    "    load_dict_from_json(filepath=\"data/test/worst_ranked_clients_df.json\")\n",
    ")\n",
    "top_purchases_by_gender_df = pd.DataFrame(\n",
    "    load_dict_from_json(filepath=\"data/test/top_purchases_by_gender_df.json\")\n",
    ")\n",
    "ranked_product_category_df = pd.DataFrame(\n",
    "    load_dict_from_json(filepath=\"data/test/ranked_product_category_df.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import demo_preparation_modeling_pipelines\n",
    "\n",
    "# Build the response dictionary\n",
    "data_dict, stats_dict = demo_preparation_modeling_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other modules not related to PySpark\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from datetime import *\n",
    "import statistics as stats\n",
    "\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100\n",
    "InteractiveShell.ast_node_interpurchase = \"all\"\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark related modules\n",
    "from utils.data_exploration import init_spark\n",
    "\n",
    "spark = init_spark(MAX_MEMORY=\"4G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_FILE = \"data/purchase_data.xltx\"\n",
    "INPUT_FILE = \"data/purchase_data_sample.xlsx\"\n",
    "\n",
    "# Load the main data set into pyspark data frame\n",
    "df = pd.read_excel(INPUT_FILE)\n",
    "spark_df = spark.createDataFrame(df)\n",
    "print(\"Data frame type: \" + str(type(spark_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_FILE = \"data/purchase_data_sample.xlsx\"\n",
    "\n",
    "# # save sample data\n",
    "# save_sample_data(df, INPUT_FILE, nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data frame stats (string and numeric columns only):\")\n",
    "spark_df.describe().toPandas()\n",
    "print(f\"There are total {spark_df.count()} row, Lets show 5 rows:\")\n",
    "spark_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.orderBy(\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Schema  & datatypes\n",
    "   *The data columns format (bigint, timestamp, double, string) and columns made of signle values not arrays/list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Columns overview\")\n",
    "spark_df.printSchema()\n",
    "pd.DataFrame(spark_df.dtypes, columns=[\"Column Name\", \"Data type\"]).set_index(\n",
    "    [\"Column Name\"]\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import categorize_columns\n",
    "\n",
    "# categorise the different columns\n",
    "string_columns, numeric_columns, array_columns, timestamp_columns, unkown_columns = (\n",
    "    categorize_columns(spark_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import count_missing_invalid_values\n",
    "\n",
    "# count the missing values\n",
    "count_missing_invalid_values(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1 replace missing  values\n",
    "\n",
    "- replace the 18% missingvalues of `Returns` by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Replace 0 for null on only population column\n",
    "# spark_df=spark_df.na.fill(value=0,subset=[\"Returns\"])\n",
    "\n",
    "# # count the missing values\n",
    "# count_missing_values(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import plot_columns, generate_explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot the target variation\n",
    "plot_columns(spark_df, x_column=\"target\", y_columns=[\"date\", \"ids\"], subplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.2 remove the unvalid value (negative price, quantity, others)\n",
    "- remove the 18% negative quantities  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_unvalid_values = (\n",
    "    spark_df.select(\"*\").where((f.col(\"target\") > 3) | (f.col(\"target\") < 0)).count()\n",
    ")\n",
    "total_nb_samples = spark_df.count()\n",
    "\n",
    "if nb_unvalid_values >= 0:\n",
    "    print(\n",
    "        f\"- {nb_unvalid_values}/{total_nb_samples} unvalid (negative) values are removed from the database \"\n",
    "    )\n",
    "    spark_df = spark_df.select(\"*\").where(\n",
    "        (f.col(\"target\") <= 3) & (f.col(\"target\") >= 0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_sample_count_sdf = (\n",
    "    spark_df.select(spark_df.target, spark_df.ids)\n",
    "    .distinct()\n",
    "    .groupBy(spark_df.target)\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "# Top 5 purchase types\n",
    "sentiment_stats_df = sentiment_sample_count_sdf.toPandas()\n",
    "# Rename column name : \"count\" --> Clients count\n",
    "sentiment_stats_df.rename(columns={\"count\": \"samples count\"}, inplace=True)\n",
    "sentiment_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sentiment \"0\"\n",
    "spark_df.filter(spark_df.target == \"0\").limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sentiment \"4\"\n",
    "spark_df.filter(spark_df.target == \"4\").limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for reading my work, wish you strong and stay safe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYNTAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /app\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"data/sentiment140/training.1600000.processed.noemoticon.csv\"\n",
    "OUTPUT_FILE = \"data/customer_totals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls artifacts/data_ingestion/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialize a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark related modules\n",
    "import pyspark\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Initialize a spark session.\n",
    "MAX_MEMORY = \"4G\"\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "    .setMaster(\"local[*]\")\n",
    "    .set(\"spark.executor.heartbeatInterval\", 10000)\n",
    "    .set(\"spark.network.timeout\", 10000)\n",
    "    .set(\"spark.core.connection.ack.wait.timeout\", \"3600\")\n",
    "    .set(\"spark.executor.memory\", MAX_MEMORY)\n",
    "    .set(\"spark.driver.memory\", MAX_MEMORY)\n",
    ")\n",
    "# initait ethe sperk session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ETL with Spark RDD\").config(conf=conf).getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract\n",
    "from utils.spark_utils import spark_load_data\n",
    "\n",
    "# Load the main dataset into pyspark data frame\n",
    "df = spark.read.csv(INPUT_FILE, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Schema  & datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The header is missing. thus we need to add it to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the coulmns names\n",
    "new_columns_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = df.toDF(*new_columns_names)\n",
    "# sort rows by date\n",
    "df = df.orderBy(\"date\")\n",
    "\n",
    "# show data schema and samples\n",
    "df.printSchema()\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 clean the data: missing value, invalid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with faulty or missing records\n",
    "df_cleaned = (\n",
    "    df.dropna()\n",
    ")  # subset=['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'UnitPrice', 'CustomerID'])\n",
    "\n",
    "\n",
    "# display message\n",
    "initial_nb_samples = df.count()\n",
    "cleaned_nb_samples = initial_nb_samples - df_cleaned.count()\n",
    "if cleaned_nb_samples > 0:\n",
    "    print(f\"- {cleaned_nb_samples}/{initial_nb_samples} samples were cleaned!\")\n",
    "else:\n",
    "    print(f\"-  the data does not need cleaning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the data to timestamp format from `Fri Apr 17 20:30:31 PDT 2009` to `2009-04-17 20:30:31`using `to_timestamp` from [spark sql-ref-datetime-pattern](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data type to a col\n",
    "# # Spark might give different results based on the version. Thus, \"LEGACY\" is needed to ensure correct results\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "df_cleaned = df_cleaned.withColumnRenamed(\"date\", \"date_original_format\")\n",
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"timestamp\", f.to_timestamp(\"date_original_format\", \"E MMM dd HH:mm:ss z yyyy\")\n",
    ")\n",
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"date\", f.to_date(\"date_original_format\", \"E MMM dd HH:mm:ss z yyyy\")\n",
    ")\n",
    "df_cleaned.printSchema()\n",
    "df_cleaned.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"label\",\n",
    "    f.when(f.col(\"target\") == 0, f.lit(\"negative sentiment\")).otherwise(\n",
    "        f.lit(\"positive sentiment\")\n",
    "    ),\n",
    ")\n",
    "df_cleaned.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 analyze the tweets sentiments trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiments_trend = df_cleaned.groupBy(\"date\").count()#.orderBy(\"label\", ascending=False)\n",
    "# sentiments_trend.show(10)\n",
    "\n",
    "sentiments_trend = (\n",
    "    df_cleaned.select(df_cleaned.date, df_cleaned.label)\n",
    "    .groupBy(df_cleaned.label)\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "sentiments_trend.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sentiments_trend = (\n",
    "    df_cleaned.groupBy(\n",
    "        f.col(\"date\"),\n",
    "        f.year(\"date\").alias(\"year\"),\n",
    "        f.month(\"date\").alias(\"month\"),\n",
    "        f.day(\"date\").alias(\"day\"),\n",
    "        f.col(\"label\"),\n",
    "    )\n",
    "    .agg(f.count(\"target\").alias(\"number_tweeks\"))\n",
    "    .orderBy(\"year\", \"month\", \"day\")\n",
    ")\n",
    "monthly_sentiments_trend.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for tag in [\"positive sentiment\", \"negative sentiment\"]:\n",
    "    df_tag = monthly_sentiments_trend.filter(f.col(\"label\") == tag)\n",
    "    date_x = df_tag.select(\"date\").toPandas().values\n",
    "    number_tweeks = df_tag.select(\"number_tweeks\").toPandas().values\n",
    "    plt.plot(date_x, number_tweeks, label=tag)\n",
    "\n",
    "plt.legend()\n",
    "plt.xticks(rotation=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "from py4j.java_gateway import JavaObject\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "\n",
    "def configure_hadoop(spark: SparkSession):\n",
    "    hadoop = spark.sparkContext._jvm.org.apache.hadoop  # type: ignore\n",
    "    conf = hadoop.conf.Configuration()\n",
    "    fs = hadoop.fs.FileSystem.get(conf)\n",
    "    return hadoop, conf, fs\n",
    "\n",
    "\n",
    "def ensure_exists(spark: SparkSession, file: str):\n",
    "    hadoop, _, fs = configure_hadoop(spark)\n",
    "    if not fs.exists(hadoop.fs.Path(file)):\n",
    "        out_stream = fs.create(hadoop.fs.Path(file, False))\n",
    "        out_stream.close()\n",
    "\n",
    "\n",
    "def delete_location(spark: SparkSession, location: str):\n",
    "    hadoop, _, fs = configure_hadoop(spark)\n",
    "    if fs.exists(hadoop.fs.Path(location)):\n",
    "        fs.delete(hadoop.fs.Path(location), True)\n",
    "\n",
    "\n",
    "def get_files(spark: SparkSession, src_dir: str) -> List[JavaObject]:\n",
    "    \"\"\"Get list of files in HDFS directory\"\"\"\n",
    "    hadoop, _, fs = configure_hadoop(spark)\n",
    "    ensure_exists(spark, src_dir)\n",
    "    files = []\n",
    "    for f in fs.listStatus(hadoop.fs.Path(src_dir)):\n",
    "        if f.isFile():\n",
    "            files.append(f.getPath())\n",
    "    if not files:\n",
    "        raise ValueError(\"Source directory {} is empty\".format(src_dir))\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def copy_merge_into(\n",
    "    spark: SparkSession, src_dir: str, dst_file: str, delete_source: bool = True\n",
    "):\n",
    "    \"\"\"Merge files from HDFS source directory into single destination file\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        src_dir: path to the directory where dataframe was saved in multiple parts\n",
    "        dst_file: path to single file to merge the src_dir contents into\n",
    "        delete_source: flag for deleting src_dir and contents after merging\n",
    "\n",
    "    \"\"\"\n",
    "    hadoop, conf, fs = configure_hadoop(spark)\n",
    "\n",
    "    # 1. Get list of files in the source directory\n",
    "    files = get_files(spark, src_dir)\n",
    "\n",
    "    # 2. Set up the 'output stream' for the final merged output file\n",
    "    # if destination file already exists, add contents of that file to the output stream\n",
    "    if fs.exists(hadoop.fs.Path(dst_file)):\n",
    "        tmp_dst_file = dst_file + \".tmp\"\n",
    "        tmp_in_stream = fs.open(hadoop.fs.Path(dst_file))\n",
    "        tmp_out_stream = fs.create(hadoop.fs.Path(tmp_dst_file), True)\n",
    "        try:\n",
    "            hadoop.io.IOUtils.copyBytes(\n",
    "                tmp_in_stream, tmp_out_stream, conf, False\n",
    "            )  # False means don't close out_stream\n",
    "        finally:\n",
    "            tmp_in_stream.close()\n",
    "            tmp_out_stream.close()\n",
    "\n",
    "        tmp_in_stream = fs.open(hadoop.fs.Path(tmp_dst_file))\n",
    "        out_stream = fs.create(hadoop.fs.Path(dst_file), True)\n",
    "        try:\n",
    "            hadoop.io.IOUtils.copyBytes(tmp_in_stream, out_stream, conf, False)\n",
    "        finally:\n",
    "            tmp_in_stream.close()\n",
    "            fs.delete(hadoop.fs.Path(tmp_dst_file), False)\n",
    "    # if file doesn't already exist, create a new empty file\n",
    "    else:\n",
    "        out_stream = fs.create(hadoop.fs.Path(dst_file), False)\n",
    "\n",
    "    # 3. Merge files from source directory into the merged file 'output stream'\n",
    "    try:\n",
    "        for file in files:\n",
    "            in_stream = fs.open(file)\n",
    "            try:\n",
    "                hadoop.io.IOUtils.copyBytes(\n",
    "                    in_stream, out_stream, conf, False\n",
    "                )  # False means don't close out_stream\n",
    "            finally:\n",
    "                in_stream.close()\n",
    "    finally:\n",
    "        out_stream.close()\n",
    "\n",
    "    # 4. Tidy up - delete the original source directory\n",
    "    if delete_source:\n",
    "        delete_location(spark, src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write outputs to csv\n",
    "TMP_OUTPUT_DIR = os.path.join(os.path.dirname(OUTPUT_FILE), \"tmp\")\n",
    "# df_cleaned.write.csv(OUTPUT_FILE)\n",
    "# print(f\"\\n - cleaned data is saved in parts to: \\n{OUTPUT_FILE}\")\n",
    "\n",
    "# your dataframe (repartitioning for demo purposes only)\n",
    "df_cleaned = df_cleaned.repartition(5)\n",
    "\n",
    "# write headers first (required for csv only)\n",
    "headers = spark.createDataFrame(\n",
    "    data=[[f.name for f in df.schema.fields]],\n",
    "    schema=T.StructType(\n",
    "        [T.StructField(f.name, T.StringType(), False) for f in df.schema.fields]\n",
    "    ),\n",
    ")\n",
    "headers.write.csv(TMP_OUTPUT_DIR)\n",
    "\n",
    "\n",
    "# write csv headers to output file first\n",
    "copy_merge_into(\n",
    "    spark,\n",
    "    TMP_OUTPUT_DIR,\n",
    "    OUTPUT_FILE,\n",
    "    delete_source=True,\n",
    ")\n",
    "\n",
    "# Write main outputs\n",
    "# dataframe written to TMP_OUTPUT_DIR folder in 5 separate csv files (one for each partition)\n",
    "df_cleaned.write.csv(TMP_OUTPUT_DIR)\n",
    "print(f\"\\n - cleaned data is saved in parts to {TMP_OUTPUT_DIR}\")\n",
    "\n",
    "# merge main csv files in folder into single file\n",
    "copy_merge_into(\n",
    "    spark,\n",
    "    TMP_OUTPUT_DIR,\n",
    "    OUTPUT_FILE,\n",
    "    delete_source=True,\n",
    ")\n",
    "print(f\"\\n - cleaned data is saved as one file to {OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
